{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, data scientists will work with enormous types of file. Having the knowledge of these types of file and importing them properly will speed up the process of analyzing big amount of data.\n",
    "In this post I will show how to import various types of data for anylyzing purpose using Python. Let's get into the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I am importing some of the Python modules I will be using throughout the tutorial as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with flat files\n",
    "In this part we will import Flat files such as txt, csv and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will open a text file with read mode and then we are reading the file using read() method. Finally we are printing the file. Quite Easy!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\tPercent\n",
      "99\t0.067\n",
      "99\t0.133\n",
      "99\t0.067\n",
      "99\t0\n",
      "99\t0\n",
      "0\t0.5\n",
      "0\t0.467\n",
      "0\t0.857\n",
      "0\t0.5\n",
      "0\t0.357\n",
      "0\t0.533\n",
      "5\t0.467\n",
      "5\t0.467\n",
      "5\t0.125\n",
      "5\t0.4\n",
      "5\t0.214\n",
      "5\t0.4\n",
      "10\t0.067\n",
      "10\t0.067\n",
      "10\t0.333\n",
      "10\t0.333\n",
      "10\t0.133\n",
      "10\t0.133\n",
      "15\t0.267\n",
      "15\t0.286\n",
      "15\t0.333\n",
      "15\t0.214\n",
      "15\t0\n",
      "15\t0\n",
      "20\t0.267\n",
      "20\t0.2\n",
      "20\t0.267\n",
      "20\t0.437\n",
      "20\t0.077\n",
      "20\t0.067\n",
      "25\t0.133\n",
      "25\t0.267\n",
      "25\t0.412\n",
      "25\t0\n",
      "25\t0.067\n",
      "25\t0.133\n",
      "30\t0\n",
      "30\t0.071\n",
      "30\t0\n",
      "30\t0.067\n",
      "30\t0.067\n",
      "30\t0.133\n"
     ]
    }
   ],
   "source": [
    "# opening \n",
    "file = open('data/seaslug.txt', mode='r')\n",
    "\n",
    "# read a file\n",
    "myFile = file.read()\n",
    "print(myFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we read the whole file at once. We can even read file line by line as well. The readline() method do the job for us easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\tPercent\n",
      "\n",
      "99\t0.067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing text files line by line\n",
    "with open('data/seaslug.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use numpy to import data as well. Here we are importing csv file using numpy. In numpy loadtext() function we are passing\n",
    "two arguments - the file and the delimter which is ',' for csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing data with numpy\n",
    "\n",
    "# Import package\n",
    "import numpy as np\n",
    "\n",
    "# Assign filename to variable: file\n",
    "file = 'data/mnist_kaggle_some_rows.csv'\n",
    "\n",
    "# Load file as array: digits\n",
    "digits = np.loadtxt(file, delimiter=',')\n",
    "\n",
    "# Select and reshape a row\n",
    "im = digits[9, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data (matplotlib.pyplot already loaded as plt)\n",
    "plt.imshow(im_sq, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to import data without column name or title. We can do so easily by one more additional argument into \n",
    "numpy loadtxt() function called skiprows=1. It will skip the first row of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file without the name of the column-header\n",
    "\n",
    "file = 'data/seaslug.txt'\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.90e+01 6.70e-02]\n",
      " [9.90e+01 1.33e-01]\n",
      " [9.90e+01 6.70e-02]\n",
      " [9.90e+01 0.00e+00]\n",
      " [9.90e+01 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "# printing 5 rows to make sure first row (column name) is not there\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import file which has both numeric and text value in it. By using numpy recfromcsv() function this job can be done very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0, 3, b'male', 22., 1, 0, b'A/5 21171',  7.25  , b'', b'S')\n",
      " (2, 1, 1, b'female', 38., 1, 0, b'PC 17599', 71.2833, b'C85', b'C')\n",
      " (3, 1, 3, b'female', 26., 0, 0, b'STON/O2. 3101282',  7.925 , b'', b'S')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT_H\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:2266: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# importing file using recfromcsv funcion\n",
    "\n",
    "file = 'data/titanic_sub.csv'\n",
    "data = np.recfromcsv(file)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is the best way to import file which gives a lot of flexibility. We can import csv file by using Pandas read_csv() function.\n",
    "And then we can look into the first few line (by default 10) of data by using pandas head method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch  \\\n",
      "0            1         0       3    male  22.0      1      0   \n",
      "1            2         1       1  female  38.0      1      0   \n",
      "2            3         1       3  female  26.0      0      0   \n",
      "3            4         1       1  female  35.0      1      0   \n",
      "4            5         0       3    male  35.0      0      0   \n",
      "\n",
      "             Ticket     Fare Cabin Embarked  \n",
      "0         A/5 21171   7.2500   NaN        S  \n",
      "1          PC 17599  71.2833   C85        C  \n",
      "2  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3            113803  53.1000  C123        S  \n",
      "4            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# Assign filename: file\n",
    "file = 'data/titanic_sub.csv'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep=',', comment='#', na_values='NaN')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with spreadsheets\n",
    "\n",
    "We can import excel spreadsheet file by using pandas ExcelFile() funtion. In the following lines of code, first we are loading the\n",
    "spreadsheet by using ExcelFile() function. And then we can use sheet_names() to know no of sheets in the spreadsheet. In the following\n",
    "file it has two sheets names '2004' and '2002'. I am loading this sheets into padas dataframe usnig parse() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Names: \n",
      " ['2002', '2004'] \n",
      "\n",
      "  War(country)      2004\n",
      "0  Afghanistan  9.451028\n",
      "1      Albania  0.130354\n",
      "2      Algeria  3.407277\n",
      "3      Andorra  0.000000\n",
      "4       Angola  2.597931\n",
      "\n",
      "\n",
      "  War, age-adjusted mortality due to       2002\n",
      "0                        Afghanistan  36.083990\n",
      "1                            Albania   0.128908\n",
      "2                            Algeria  18.314120\n",
      "3                            Andorra   0.000000\n",
      "4                             Angola  18.964560\n"
     ]
    }
   ],
   "source": [
    "# importing xlsx\n",
    "\n",
    "file = 'data/battledeath.xlsx'\n",
    "\n",
    "# load spreadsheet\n",
    "data = pd.ExcelFile(file)\n",
    "\n",
    "# printing sheetnames\n",
    "print('Sheet Names: \\n',data.sheet_names, '\\n')\n",
    "\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = data.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "print('\\n')\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = data.parse('2002')\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with SAS file\n",
    "We can import sas file by using sas7bdat module. Here I am getting the file using SAS7BDAF and saving it as data frame with to_data_frame()\n",
    "method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     YEAR     P           S\n",
      "0  1950.0  12.9  181.899994\n",
      "1  1951.0  11.9  245.000000\n",
      "2  1952.0  10.7  250.199997\n",
      "3  1953.0  11.3  265.899994\n",
      "4  1954.0  11.2  248.500000\n"
     ]
    }
   ],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('data/sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with STATA file\n",
    "We can easily import stata file with pandas read_stata function. You can learn about stata from wikipedia. Below I am have one state\n",
    "dataset called disarea. I am importing this dataset using pandas read_stata funciton and printing 3 rows of data by using pandas head() \n",
    "method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wbcode      country  disa1  disa2  disa3  disa4  disa5  disa6  disa7  disa8  \\\n",
      "0    AFG  Afghanistan   0.00   0.00   0.76   0.73    0.0    0.0   0.00    0.0   \n",
      "1    AGO       Angola   0.32   0.02   0.56   0.00    0.0    0.0   0.56    0.0   \n",
      "2    ALB      Albania   0.00   0.00   0.02   0.00    0.0    0.0   0.00    0.0   \n",
      "\n",
      "    ...    disa16  disa17  disa18  disa19  disa20  disa21  disa22  disa23  \\\n",
      "0   ...       0.0     0.0     0.0    0.00     0.0     0.0    0.00    0.02   \n",
      "1   ...       0.0     0.4     0.0    0.61     0.0     0.0    0.99    0.98   \n",
      "2   ...       0.0     0.0     0.0    0.00     0.0     0.0    0.00    0.00   \n",
      "\n",
      "   disa24  disa25  \n",
      "0    0.00    0.00  \n",
      "1    0.61    0.00  \n",
      "2    0.00    0.16  \n",
      "\n",
      "[3 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# loading stata file in pandas dataframe\n",
    "file = 'data/disarea.dta'\n",
    "df = pd.read_stata(file)\n",
    "\n",
    "# printing first 3 lines\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Matlab file\n",
    "Another popular type of file is called matlab file with extension .mat at the end. We can import such file by using scipy.io module\n",
    "in Python. Here I am loading the file using scipy.io.loadmat() funcion by passing the mat file. And then checking the file type.\n",
    "As we can see it belongs to Python dict class. That means we extract the keys out of it. We can go deeper by digging the value of these keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of mat file <class 'dict'> \n",
      "\n",
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "rfpCyt\n",
      "rfpNuc\n",
      "cfpNuc\n",
      "cfpCyt\n",
      "yfpNuc\n",
      "yfpCyt\n",
      "CYratioCyt\n",
      "<class 'numpy.ndarray'>\n",
      "(200, 137)\n"
     ]
    }
   ],
   "source": [
    "# Importing matlab file\n",
    "import scipy.io\n",
    "\n",
    "# assigning filename to file\n",
    "file = 'data/ja_data2.mat'\n",
    "\n",
    "# loading matlab file\n",
    "mat = scipy.io.loadmat(file)\n",
    "\n",
    "# check type of the file\n",
    "print('Type of mat file', type(mat), '\\n')\n",
    "\n",
    "# extracting keys from mat file\n",
    "for key in mat.keys():\n",
    "    print(key)\n",
    "    \n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Database file\n",
    "\n",
    "It's quite obvious to work with databases while working in company. That's why good understanding of databases is the key skill of \n",
    "data scientist. We can use sqlalchemy to work with databases. \n",
    "\n",
    "In the following lines of code first we are importing create_engine from sqlalchemy which we will be using to establish engine and connection \n",
    "with databases. We are creating the engine by passing the appropriate argument such as the type of file we will be working with which is sqlite. As we\n",
    "know, databases are consist of tables. We can extrct the table by using table_names() method which will return a list of \n",
    "tables names avaailable in the database file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    }
   ],
   "source": [
    "# Working with databases\n",
    "\n",
    "# importing necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine('sqlite:///data/Chinook.sqlite')\n",
    "\n",
    "# creating a list with table names contains in engine\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# printing table names\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what tables are avaiable in our datadase file. It's time to execute some sqlite queries. First we establish\n",
    "connection using connect() method and then execute any query with the execute() funtion. Below we are seleting everything from\n",
    "Album table and passing it to the pandas DataFrame() funciton to create dataframe for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AlbumId                                  Title  ArtistId\n",
      "0        1  For Those About To Rock We Salute You         1\n",
      "1        2                      Balls to the Wall         2\n",
      "2        3                      Restless and Wild         2\n",
      "3        4                      Let There Be Rock         1\n",
      "4        5                               Big Ones         3\n"
     ]
    }
   ],
   "source": [
    "# opening engine connection\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query \n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# saving to dataframe\n",
    "df = pd.DataFrame(rs.fetchall(), columns=rs.keys())\n",
    "\n",
    "# close connection\n",
    "con.close()\n",
    "\n",
    "# printing our datafrmae\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the shorter version to establish connection and execute queries on database where we don't need to care about\n",
    "closing the connection. Below codeblock pretty much does the same as above except here I am executing several queries and \n",
    "capturing into different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LastName                Title\n",
      "0    Adams      General Manager\n",
      "1  Edwards        Sales Manager\n",
      "2  Peacock  Sales Support Agent\n",
      "3     Park  Sales Support Agent\n",
      "4  Johnson  Sales Support Agent\n",
      "\n",
      "\n",
      "   EmployeeId LastName FirstName                Title  ReportsTo  \\\n",
      "0           4     Park  Margaret  Sales Support Agent        2.0   \n",
      "1           2  Edwards     Nancy        Sales Manager        1.0   \n",
      "\n",
      "             BirthDate             HireDate           Address     City State  \\\n",
      "0  1947-09-19 00:00:00  2003-05-03 00:00:00  683 10 Street SW  Calgary    AB   \n",
      "1  1958-12-08 00:00:00  2002-05-01 00:00:00      825 8 Ave SW  Calgary    AB   \n",
      "\n",
      "  Country PostalCode              Phone                Fax  \\\n",
      "0  Canada    T2P 5G3  +1 (403) 263-4423  +1 (403) 263-4289   \n",
      "1  Canada    T2P 2T3  +1 (403) 262-3443  +1 (403) 262-3322   \n",
      "\n",
      "                      Email  \n",
      "0  margaret@chinookcorp.com  \n",
      "1     nancy@chinookcorp.com   \n",
      "\n",
      "                                   Title    Name\n",
      "0  For Those About To Rock We Salute You   AC/DC\n",
      "1                      Balls to the Wall  Accept\n",
      "2                      Restless and Wild  Accept\n"
     ]
    }
   ],
   "source": [
    "# better way\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT LastName, Title FROM Employee')\n",
    "    df = pd.DataFrame(rs.fetchmany(size=10), columns = rs.keys())\n",
    "    \n",
    "    rs2 = con.execute('SELECT * FROM Employee ORDER BY BirthDate')\n",
    "    df2 = pd.DataFrame(rs2.fetchall(), columns=rs2.keys())\n",
    "    \n",
    "    rs3 = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df3 = pd.DataFrame(rs3.fetchall(), columns=rs3.keys())\n",
    "  \n",
    "    \n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df2.head(2),'\\n')\n",
    "print(df3.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Pandas way of interacting with database file. We can use pandas read_sql_query() function do our job by passing the \n",
    "query we want to execute and the engine. [engine is defined eartier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeId  LastName FirstName       Title  ReportsTo            BirthDate  \\\n",
      "0           8  Callahan     Laura    IT Staff          6  1968-01-09 00:00:00   \n",
      "1           7      King    Robert    IT Staff          6  1970-05-29 00:00:00   \n",
      "2           6  Mitchell   Michael  IT Manager          1  1973-07-01 00:00:00   \n",
      "\n",
      "              HireDate                      Address        City State Country  \\\n",
      "0  2004-03-04 00:00:00                  923 7 ST NW  Lethbridge    AB  Canada   \n",
      "1  2004-01-02 00:00:00  590 Columbia Boulevard West  Lethbridge    AB  Canada   \n",
      "2  2003-10-17 00:00:00         5827 Bowness Road NW     Calgary    AB  Canada   \n",
      "\n",
      "  PostalCode              Phone                Fax                    Email  \n",
      "0    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772    laura@chinookcorp.com  \n",
      "1    T1K 5N8  +1 (403) 456-9986  +1 (403) 456-8485   robert@chinookcorp.com  \n",
      "2    T3B 0C5  +1 (403) 246-9887  +1 (403) 246-9899  michael@chinookcorp.com  \n"
     ]
    }
   ],
   "source": [
    "# we can\n",
    "df = pd.read_sql_query('SELECT * FROM Employee WHERE EmployeeId >=6 ORDER BY BirthDate', engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first post in Medium, a small step to help people with my knowlege and learn as well. Humbly speaking, this knowledge\n",
    "of importing file and the datasets I have gained from Datacamp.com. Till the next post stay well. Thanks for reading my post. Feel\n",
    "free to leave your comments on the topic and how can I imporve my writing to make people understand better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
